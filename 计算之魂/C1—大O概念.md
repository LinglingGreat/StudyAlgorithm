## 1.1算法的规范化和量化度量

图灵提出了计算机的数学模型，冯·诺依曼确定了计算机通用的系统结构，而高德纳则奠定了计算机算法的基础。

高德纳**Donald Ervin Knuth**
- 计算机算法分析的鼻祖，提出了评估计算机算法的标准
- 编写了计算机科学领域的“圣经”——《计算机程序设计艺术》
- 迄今为止最年轻的图灵奖获得者
- 写了一个排版软件TeX（后来被人做成了更方便使用的LaTeX）
- 是硅谷地区众多图灵奖获得者中名气最大、最会编程的人。总是力争一次全对，没有错误，而且算法在设计时就达到最佳。

冯·诺依曼发明计算机体系结构和高德纳编写TeX程序似乎都是偶然为之的结果。冯·诺依曼原本不想发明计算机，他只想算题；高德纳也不想发明排版软件，他只想写书。为什么这些大师们偶然为之的工作比二流人才穷其一生的发现有时还有影响力呢？因为除了能力的差异外，他们还有着**遇到问题时解决问题的积极态度**。任何人在前进的过程中都会遇到问题，但是对待问题的态度决定了个人的命运。

## 1.2大数和数量级

哪个算法更好一些？

早期科学家们的看法不统一，这个没有明确的答案。直到1965年尤里斯·哈特马尼斯（Juris Hartmanis）和理查德·斯特恩斯（Richard Stearns）提出了算法复杂度的概念（二人后来因此获得了图灵奖），计算机科学家才开始考虑用一种公平、一致的评判方法来对比不同算法的性能。最早将算法复杂度严格量化衡量的就是高德纳，他也因此被誉为“算法分析之父”。

高德纳的思想主要包括以下三个部分。

1．在比较算法的快慢时，只需要考虑数据量特别大，大到近乎无穷大时的情况。因为计算机的发明就是为了处理大量数据的，而且数据越处理越多。

2．决定算法快慢的因素虽然可能有很多，但是所有的因素都可以被分为两类：第一类是不随数据量变化的因素，第二类是随数据量变化的因素。

3．两种算法在复杂度上相差哪怕只有一点点，N很大之后，效率可能就差出万亿倍了。比如用非常容易想到的选择排序或插入排序和专业人士常用的快速排序对10多亿个QQ号排一次序，计算量分别是大约100亿亿次和30亿次。

相关笔记：[1【数据结构与算法】复杂度分析](../原理/1【数据结构与算法】复杂度分析.md)

## 1.3怎样寻找最好的算法

#### 例题1.3　总和最大区间问题　（难度系数3颗星)

给定一个实数序列，设计一个最有效的算法，找到一个总和最大的区间。

比如在下面的序列中：

1.5, −12.3, 3.2, −5.5, 23.2, 3.2, −1.4, −12.2, 34.2, 5.4, −7.8, 1.1, −4.9

总和最大的区间是从第5个数（23.2）到第10个数（5.4）。

类似题目：[53. 最大子序和_E](../explain/53.%20最大子序和_E.md)


解决这个问题有四种可行的方法，下面根据计算复杂度从高到低的次序逐一介绍。

**方法1，做一次三重循环.** 我们假设这个序列有$K$个数，目标是计算所有区间的总和。所有区间的组合$S(p,q)$有$O(K^2)$种（p可以从1一直到K，q可以从p一直到K）。在每一种组合中，计算$S(p,q)$平均要做K/4次加法，这是又一重循环。因此这种算法的复杂度是$O(K^3)$。

计算$S(p,q)$平均要做K/4次加法：

![](img/Pasted%20image%2020220831214244.png)


**方法2，做两重循环。**

方法1效率不高的原因是做了太多的无用功，存在重复计算。实际上我们只需要记录三个中间值：
- 当前位置总和S(p,q)
- 当前最大和Max。有了这个值之后，如果S(_p_,_q_+1)≤Max，则Max维持不变；如果S(p,q+1)>Max，则要更新Max，当然，我们也要记录下来Max是在区间[p,q+1]取得的。
- Max对应的区间结束的位置r

对于给定的p，需要从头到尾试K−p次，也就是O(K)的复杂度。而p可以从1到K，有K种可能性，二者的组合就是O(K^2)（省去了计算S(p,q)的复杂度，分摊到每一步了）。如果K有好几万，计算量是十几亿，而方法1的计算量是它的上万倍。如果你能想到这种方法，那就基本上达到了五级工程师的要求，因为你已经搞清楚哪些计算是重复计算了。


**方法3，利用分治（Divide-and-Conquer）算法。**

首先，将序列一分为二，分成从1到K/2(如果是奇数，就是(K-1)/2)，以及从K/2+1到K两个子序列。

然后，我们对这两个子序列分别求它们的总和最大区间。接下来有两种情况。

1．前后两个子序列的总和最大区间中间没有间隔，如果两个区间各自的和均为正整数，这时，整个序列总和最大区间就是[p, q]；否则，就选取两个子序列的总和最大区间中大的一个。

2．前后两个子序列的总和最大区间中间有间隔，我们假定这两个子序列的总和最大区间分别是[p1,q1]和[p2,q2]。这时，整个序列的总和最大区间是下面三者中最大的那一个：

（1）[p1,q1]；

（2）[p2,q2]；

（3）[p1,q2]。

为什么？不理解 #td 

比如序列[-2, 1, -3, 4, -1, 2, 1, -5, 4]就不是这样的

第三个其实是对从q1+1到p2-1之间的数字求和，复杂度为O(K)。有了上面三个值，挑出最大的一个即可。

至于每个子序列的总和最大区间如何求，可以用到递归算法，这种算法的耗时为O(KLogK)。对于几万个数据的序列，计算量为百万级，这比方法2的十几亿又小了不少（是方法2的千分之几）。如果你能够想出这种方法，那在计算机科学的理论上就具备了成为四级工程师的条件，因为你已经掌握了计算机科学的一个精髓——分治算法。当然，一个工程师合格与否要看这个人能否做出实际的产品，除了理解算法，还需要在实际工作中历练。


**方法4，正、反两遍扫描的方法。**

在方法2中，我们是先设定区间的左边界p，在此条件下确定总和最大区间的右边界q。然后再改变左边界，测试所有的可能性。但实际上，这种方法在无形中已经找到了总和最大区间的右边界。我们从这个想法出发，来寻找一下线性复杂度，即O(_K_)的算法。从前往后扫描找到右边界，从后往前扫描找到左边界。

步骤如下。

步骤1，先在序列中扫描找到第一个大于零的数，假定这个数不存在（即所有的数字非零即负），那么整个序列中最大的那个数就是所要找的区间。这时算法的复杂度是O(K)。因此，我们可以不失一般性地假设第一个数字是正数，如果这个数小于或等于零，则从序列头部删除，如此反复，最终删除序列头部连续排列的负数或零。

步骤2，我们用类似于方法2中的做法，先把左边界固定在第一个数，然后让q=2,3,…,K，计算S(1,q)，以及到目前为止的最大值Maxf和达到最大值的右边界r。

步骤3，如果对于所有的q，都有S(1,q)≥0，或者存在某个q0，当q＞q0，上述条件满足，这个情况比较简单。当扫描到最后，即q=K时，所保留的那个Maxf所对应的r就是我们要找的区间的右边界。为什么呢？因为从第r+1个数往后加，无论怎么加，都是负数（或者零），所以右边界不可能往后延长了。

这时候，我们还不知道左边界在哪里。其实只要把这个问题倒过来看就可以了。我们从后往前计算后向累计之和，结果见表1.1的第4行。用同样的方法，我们可以计算出后向累计的最大值（_Maxb_=40.8），以及达到这个数值的位置（l=5），它就是左边界，如图1.2所示。这样整个总和最大的区间就是[l,r] =[5, 10]。

为了便于大家理解，我们使用例题1.3中的数据，从前往后一步步累加计算一遍，计算结果放在了表1.1中。从算得的结果可知Maxf=39.3，相应的r=10。在相应的图1.2中，就是前向累计之和曲线上用圆圈表示的位置。

表1.1　序列中的元素、前向累计之和和后向累计之和

![](https://oss.linklearner.com/the-soul-of-calculation/table_1.1.png)


![](https://oss.linklearner.com/the-soul-of-calculation/1.2.png)

图1.2　序列中元素的值、前向累计之和以及后向累计之和

如果一个工程师能够想出这种方法，就具有成为三级工程师的潜力了，因为他能完全理解在解决问题时哪些计算是必需的、不能省略的，哪些则是多余的。所谓提高一台计算机软硬件的效率，就是将多余的计算全部挑出来省掉。

在这个问题中，如果S(1,q)在某个地方小于零，然后就一直小于零，这个事情就变得比较麻烦了。比如我们将上面的那组数据改动两个，如表1.2所示，这时如果我们直接采用前面步骤3的方法就会出现问题。

表1.2　改动后的元素、前向累计之和和后向累计之和

![](https://oss.linklearner.com/the-soul-of-calculation/table_1.2.png)

从表1.2中可以看出，从前往后累加最大值出现在r=6的位置，而反过来从后往前累加，最大值出现在l=9的位置。右边界反而在左边界的左边。上述算法显然要出错。造成这个问题的原因是从一开始累加的总和在遇到第8个元素时下跌到零以下，然后一直在零以下。这样一来，原本区间[9, 10]之间的元素之和为49.6，它应该是总和最大区间，但是在累加了前8个元素之后和依然小于零，因此我们找不到，如图1.3所示。

![](https://oss.linklearner.com/the-soul-of-calculation/1.3.png)

图1.3　前向累计之和在某个位置之后就一直小于零的情况，其峰值在后向累计之和峰值之前

为了解决这个问题，我们需要对步骤2和步骤3稍作改进。

步骤2′，我们先把左边界固定在第一个大于零的位置，假设为p，然后让q=p,p+1,…,K，计算S(p,q)，以及到目前为止的最大值Max和达到最大值的右边界r。如果我们算到某一步时，发现S(p,q)<0，这时，我们需要从位置q开始，反向计算Maxb，并且可以确定从第1个数到第q个数之间和最大的区间，我们假定它为[l1,r1]，这个区间的和为Max1。

特别值得指出的是，l1其实等于p。为什么呢？如果l1≠p，根据我们对这种情况的假设，S(p,l1−1)≥0，于是就有S(_p_,_r1)=S_(_p_,_l_1−1)+S(l_1,r_1)≥S_(l_1,r_1)=Max_1，这就与[l1,r1]是到q为止和最大的区间相矛盾了。

步骤3′，我们从q+1开始往后扫描，重复上述过程。先是找到第一个大于0的元素，从那里开始做累加操作，可能在遇到某个q′时，又出现S(_q_+1,_q_′)<0的情况了，这时我们得到第二个局部和最大区间[l_2,r_2]，相应的区间之和为Max_2。

现在，我们需要确定，从头开始到q′时和最大的区间。我们只需要比较一下Max_1、Max_2和Max_1+Max_2+S(r_1+1,l_2−1)［也就是S(l_1,r_2)］这三个数值，最大的区间和必然在这三者之间。

我们先否定掉S(l_1,r_2)的可能性。

由于$S(q+1,r_2)=S(q+1,l_2−1)+S(l_2,r_2)<S(l_2,r_2)$，故

$S(q+1,l_2−1)<0$ 　　（1.1）

也就是说从第一次累加结束，到第二个局部和最大区间开始之前，中间所有的元素之和小于0。同时，由于

$S(l_1,r_1)+S(r_1+1,q)=S(p,r_1)+S(r_1+1,q)=S(p,q)<0$ 　　（1.2）

综合不等式（1.1）和不等式（1.2），我们就得到

$Max_1+S(r_1+1,l_2−1)=S(l_1,r_1)+S(r_1+1,q)+S(q+1,l_2−1)<0$　　（1.3）

也就是说

$S(l_1,r_2)=Max_1+Max_2+S(r_1+1,l_2−1)<Max_2$　　（1.4）

这样一来，从序列头开始到q′时和最大的区间要么是[l_1,r_1]，要么是[l_2,r_2]，不可能在这两段之间，我们只要将二者之中更大的区间保留到中间变量Max和[l,r]中即可。

步骤4，采用与步骤3′同样的方法，不断往后扫描整个序列，得到一个个局部和最大的区间[l_i,r_r]和相应的部分和Max_i，然后比较Max_i和Max，决定是否更新Max。

最后，这样得到的局部和最大区间[l,r]，就是整个序列的总和最大区间。

为了让大家对简单情况和复杂情况有更直观的印象，下面再用两个极端一点的例子来说明。

在图1.4中，第一个序列［见图1.4（a）］从前往后的累计之和虽然是上下波动的，但是总大于0，因此我们从头扫到尾就能确定总和最大区间的右边界。在第二个序列中［见图1.4（b）］，中间有一次或者若干次累计之和跌到了零以下，这实际上把整个序列分成了几段，每一段（子序列）都符合简单的情况。我们只需要对比一下每一段的总和最大区间就可以了。那么为什么整个序列的总和最大区间不会跨在两个子序列之间呢？因为当每一次累计总和跌到零以下时，说明在两个局部总和最大区间之间的元素加进来只会让总和变小，而不是变大。
  
![](https://oss.linklearner.com/the-soul-of-calculation/1.4.png)

图1.4　序列在累计求和时出现的简单情况和复杂情况的对比

无论是简单的情况还是复杂的情况，这个算法只需要扫描整个序列两遍（从头到尾，再从尾到头），因此它的复杂度只有O(_K_)。对于几万个数字，它的计算量只有几万次而已，比方法3又快了几十倍。

从方法1到方法4，我们将一个问题的计算量从几十万亿降低到几万。在计算机科学中有时没有标准答案或者最佳答案，却有好的方法和平庸的方法之分。虽然它们有些时候看似差不太多，但是在计算机上运行的效率差距常常是巨大的，大到很多个数量级。由此可见，从业者在水平上微小的差异，会导致他们采用不同的方法做事情，而结果就有天壤之别了。这就是为什么一流工程师的贡献会是二流工程师的10倍。因此，如果你想在计算机行业中往上走，要不断提升自己才有出路。

补充：方法5，动态规划。


这道例题是AB、MS等公司的面试题。

大约30%的面试者只想出了方法1，如果面试AB公司，这个问题会得到0分。

大约40%的面试者能想出方法2，按照AB公司的标准（按4分制），可以得到2分。2分是不及格。

15%～20%的面试者能想出方法3，或者复杂度为O(NlogN)的类似方法，可以得3分左右。在AB公司，如果其他问题的表现也是3分，可能依然无法通过面试。如果只有一两个面试官给3分，其他面试官打分在3分以上，大致可以通过面试。

大约10%的面试者能够想出线性复杂度的解法，而其中大约有一半的面试者能够想出复杂情况的算法，另一半只能想清楚简单情况。不过，即便只能想清楚简单情况，能够将算法表述清楚，也可以得3.3～3.5分，如果面试者在所有面试官那里都得了这个成绩，就直接被录用了。当然，能够把这个问题的复杂情况想清楚的人，具有其他人所不具备的两个长处：首先，他考虑问题很周全，当好科学家有时可以靠聪明和灵感，但是当好工程师，还需要考虑问题周全；其次，他有一个非常清晰的头脑，能够把复杂的问题想清楚。这样的人具有成为2.5级工程师的潜力。当然，能否走到那一步，还需要很多其他条件。

这个问题对学习计算机的人来讲是一个很好的练习题。它不仅可以帮助大家理解不同算法在复杂度上的差异，而且可以让大家不断深入思考和寻找更好的答案并且想清楚各种细节。很多人靠刷题学习计算机算法的原理，这种办法只对解决那些有明确而简单答案的问题有效。真正能够将计算机算法灵活应用的人，都需要领悟计算机科学的精妙之处，而这道例题则能够帮助大家领悟计算机科学的精髓。

在计算机科学领域，从一个能够解决问题的从业者，上升到一个能找到最佳解决方案的人，需要培养对计算机科学的感觉。对于这个问题，有经验的从业者一开始就能够大致判断出它一定有优于平方复杂度［即O(N^2)］的解法。这样，他们才会直接朝这个方向努力。这样的感觉如何建立呢？就这个问题，下面分享三点个人体会。

- 首先是对一个问题边界的认识。在这道例题中，我们知道至少要扫描整个序列一次，因此最优解法的下界不可能低于线性复杂度。

- 其次，在计算机科学中，优化算法最常用的方法就是检查一种算法是否在做大量无用功。在上述问题中，O(N^2)复杂度的算法会把每一个元素扫描N次，并且做N次加法，这显然是无用功，比如加第i个元素，加多少次结果都是同样的。而线性复杂度的算法，确定区间的一个边界只需要对每一个元素做一次加法，这就省去了无用功。

- 最后，我们需要逆向思维。在这个问题中，如果我们已知总和最大区间的左边界，只需要寻找右边界，很容易通过一次扫描完成。事实上，在面试AB公司的人中，一多半人能够做到这一点。绝大部分人卡壳是卡在不知道该如何确定总和最大区间的左边界。这个问题其实我们只要把给定的序列倒过来，从后往前看，就迎刃而解了。

关于逆向思维，我们在后面还会多次讲到，因为它对于计算机行业的从业者非常重要。**人通常是喜欢从前到后顺着想问题，不喜欢反过来从后往前思考；喜欢做加法、做乘法，而不喜欢做减法、做除法；喜欢从小到大看、从下往上归纳，而不喜欢从大往小看、从上往下演绎。** 有些很简单的问题，正向思维难以找到答案，而逆向思维却马上迎刃而解。人的思维很多时候和计算机科学应有的思维是矛盾的，要成为一流的计算机科学家或工程师，需要有意识地改变自己的思维方式，突破常规。


## 1.4关于排序的讨论

选择排序，插入排序，归并排序，

相关笔记：[排序算法](../原理/排序算法.md)

选择排序（Selection Sort）：
- 每一次从序列中挑出一个最大值，放在序列的最后，这样重复多次扫描序列后，整个序列就排序完毕。
- 复杂度O(N^2)

插入排序（Insert Sort）：
- 对于未排序数组，我们不断从后向前扫描，这就相当于从后向前摸牌，对于每一个拿到手上的元素，我们找到相应的位置插入。最后所有的元素扫描一遍，全部插入相应的位置，也就实现了排序。
- 复杂度O(N^2)
- 找到插入的位置可以采用二分查找，在log_N_的时间内完成，但是挪动元素的时间省不了，最糟糕的情况是插入一个元素，要挪动_N_−1个元素。


我们不妨以选择排序为例，看看哪些操作是在做无用功。

首先，选择排序将所有的数字都两两比较了一次，这其实没有必要，因为如果已经比较出X<Y、Y<Z，就没有必要再比较X和Z了。

其次，选择排序做了很多无谓的位置互换。举一个极端的例子，如果数组a已经逆序排好了，也就是说a_[1]最大，a_[2]次之，a_[_N_]最小，a_[1]和a_[2]的有效移动都应该是往后移。但是，第一遍扫描时，先将a_[2]往前移到了第一个位置，这是个无用功。在一个序列一开始次序是完全随机的状态下，排序时这种无用的位置互换非常多。

**效率不高的算法的主要问题是存在大量的甚至重复的无用功，而提高算法效率，就需要分析哪些计算是不可或缺的、哪些是无用功。**

归并排序（Merge Sort）、快速排序（Quick Sort）和堆排序（Heap Sort）。这三种算法的共同特点是平均时间复杂度均为O(_N_log_N_)。

归并排序
- 冯·诺依曼在1945年发明
- （a）两个序列当前最小的值相比较，更小的一个加入合并后的序列中；（b）当一个序列被合并完，另一个序列的剩余部分直接加入合并后的序列中
- 归并排序有一个问题，就是它需要使用额外的存储空间保留中间结果，因为当我们把_B_和_C_这两个子序列合并为_A_序列时，需要额外的_O_(_N_)大小的存储空间。

堆排序
- 在1964年，加拿大计算机科学家约翰·威廉斯（John W. J. Williams）提出
- 不占用额外的空间，这种特性被称为就地特征（in place characteristic）。
- 不具有稳定性（两个相同的元素在排序前后相对位置维持原有的次序）

在找到时间复杂度为_O_(_N_log_N_ )的排序算法后，照说在数量级上不可能再有更好的排序算法了（这一点我们会在本章的附录里证明），但是毕竟相同数量级的算法仍可能有常数倍的差异。因此虽然在算法复杂度上寻找同数量级算法是一件没有意义的事情，但是如果在工程上能保证某个算法的实际运行时间总是比其他算法少一些还是有人愿意研究的。

快速排序
- 英国计算机科学家托尼·霍尔（Tony Hoare）发明了一种比归并排序算法和（后来的）堆排序算法快两三倍的算法，他称之为快速排序算法。
- 快速排序算法只需要O(log_N_ )的额外空间，虽然这让它不满足就地特征的要求，但是这个空间需求足够小，小到可以忽略。
- 不具有稳定性
- 虽然它的平均时间复杂度是O(_N_log_N_)，但是在极端的情况下时间复杂度是O(N^2)。虽然后来不少计算机科学家通过改进让它的最坏时间复杂度也能达到O(N_log_N)，但是效率就有所下降。可见，在计算机领域，任何改进其实都是有代价的。

我们可以看出这三种排序算法各有千秋，这里体现出在计算机科学领域做事的两个原则：首先，要尽可能地避免那些做了大量无用功的方法，比如选择排序和插入排序，一旦不小心采用了那样的方法，带来的危害有时是灾难性的；其次，接近理论最佳值的算法可能有很多种，除了单纯考量计算时间外，可能还有很多考量的维度，因此有时不存在一种算法就比另一种绝对好的情况，只是在设定的边界条件下，某些算法比其他的更适合罢了。

### 蒂姆排序
科学家们依然在考虑在某个特定的应用中寻找一些更好的排序算法，当然使用一种排序算法可能难以兼顾前面讲到的各个维度的多种需求。因此，今天人们对排序算法的改进大多是结合几种排序算法的思想，形成混合排序算法（Hybrid Sorting Algorithm），比如将快速排序和堆排序结合起来的内省排序（Introspective Sort，简称Introsort），它成为如今大多数标准函数库（STL）中的排序函数使用的算法，还有接下来要介绍的蒂姆排序（Timsort），它是今天Java和安卓（Android）操作系统内部使用的排序算法。

> 蒂姆排序这个名字来源于该算法的发明人蒂姆·彼得斯（Tim Peters）。他在2002年发明了一种将两种排序算法的特点相结合（插入排序节省内存、归并排序节省时间），最坏时间复杂度控制在_O_(_N_log_N_)量级，同时还能够保证排序稳定性这样一举三得的混合排序算法。蒂姆排序最初是在Python程序语言中实现的，今天它依然是这种程序语言默认的排序算法。

蒂姆排序可以被看成是以块（它在算法中被称为run）为单位的归并排序，而这些块内部的元素是排好序的（无论是从小到大，还是从大到小排序均可）。我们回顾一下前面的例题1.3，任何一个随机序列内部通常都有很多递增（从小到大）的子序列或者递减（从大到小）的子序列,相邻两个数总是一大一小交替出现的情况并不多.

蒂姆排序就是利用了数据的这个特性来减少排序中的比较和数据移动的，它的大致思想如下。

步骤1，找出序列中各个递增和递减的子序列。如果这样的子序列太短，小于一个预先设定的常数（通常是32或者64），则用简单的插入排序将它们整理为有序的子序列（也称块，run）。在寻找插入位置时，该算法采用了二分查找。随后将这些有序子序列一个一个放入一个临时的存储空间（堆栈）中

步骤2，按照规则合并这些块。合并的过程是先合并两个最短的，而不是一长一短地合并，可以证明这样效率会高些。合并的方法从原理上讲和归并排序中两个子序列的合并是相同的，但是为了提高效率，算法中所说的块，其实都可以通过批处理的方式进行归并，而不需要一个个地进行。例如图1.8所展示的两个子序列在合并时，可以直接将X序列中的前四个数（7，9，13，16）一次加到Y序列的3之后，类似的，可以把Y序列中的三个数（33，36，37）直接归并到X序列的19之后。这样成组归并的数字在图中用[ ]表示。当然，能够成批归并的前提是知道这些组的边界。比如当Y序列中的3首先进入归并后的序列之后，接下来不仅要比较X中的7和Y中的17，而且需要知道在X序列中大于17的数字的位置（第5个，19）。如果一个个顺序扫描，就和传统的归并排序算法没有区别了。蒂姆排序中采用的是一种跳跃式（galloping）预测的方式，至于具体怎么跳跃，有兴趣的读者可以参看参考资料https://github.com/python/cpython/blob/master/Objects/listsort.txt     #td 

  
![](https://oss.linklearner.com/the-soul-of-calculation/1.8.png)

图1.8　蒂姆排序成块插入

显然，蒂姆排序的时间复杂度不会比归并排序更高，因此它的上限是_O_(_N_log_N_)。但是在实际运行时蒂姆排序要比归并排序快几倍。不少人使用了完全随机的序列对蒂姆排序进行测试，结论是它的速度和快速排序基本相当。由于它是一种稳定的排序算法，便于多列列表的排序，因此今天应用非常广泛。

作为一个计算机从业者，其实是否了解蒂姆排序的细节不是很重要，重要的是通过它理解如何掌握计算机科学的精髓，对各种算法做到运用之妙，存乎一心。蒂姆排序的妙处在于，它非常灵活地利用了插入排序简单直观以及归并排序效率高的特点，并且找到了归并排序的一些可以进一步提高的地方，即归并过程中过多地一对一比较大小。如果我们回顾一下前面所说的少做无用功这个原则，就能慢慢摸到算法设计的窍门了。**能够在解决实际问题时自觉应用蒂姆排序的那些原则，就有了成为3～2.5级工程师的潜力**。


## 附录　为什么排序算法的复杂度不可能小于O(NlogN)

对于这个问题，我们需要换一种思路来思考。假定一个数组有N个元素，我们来看看对它排序最少需要做多少次的比较。显然排序耗时一定会超过这些比较所花的时间。

假定有两个序列a_1,a_2,…,a_i,…,a_N和b_1,b_2,…,b_i,…,b_N，我们需要对它们的大小进行比较。规则是这样确定的：假如a_i和b_i是第一对不同的元素，且a_i≤b_i，而它们前面的元素都相同，即a_1=b_1,a_2=b_2,…,a_i−1=b_i−1，那么我们就说第一个序列小于第二个序列。

对于任意一个序列a_1,a_2,…,a_N，假如随意排列其中的元素，可以排出很多种序列，则这些排列中，最小的序列是将其中每一个元素从小到大排好序的那个序列。在所有可能的排列组合中，通过元素的比较挑出最小的一个，就是排序。

接下来，我们来看看比较M个序列的大小需要做多少次元素之间的比较。

假定有两个序列，它们除了在第i个和第j个位置上的元素彼此互换，其中i<j，其他元素都相同，即这两个序列可以写为a_1,a_2,…,a_i,…,a_j,…,a_N和a_1,a_2,…,a_j,…,a_i,…,a_N。如果a_i≤a_j，第一个序列就小于第二个序列；反之则第二个序列小于第一个序列。也就是说，将两个元素a_i和a_j做一次比较，我们最多能区分出两个不同序列的大小。

如果我们进行两次比较，最多能够区分出多少个序列的大小呢？显然最多是四种。类似地，假如我们做k次比较，最多能区分出2^k种不同序列的大小。反过来，如果我们有M种序列，要区分出它们的大小，需要logM次比较。

接下来我们思考一下，N个元素的数组能排出多少种可能的序列呢？显然是N!种。因此要区分出这么多种序列的大小，挑出最小的一个，至少需要logN!次比较，如图1.9所示。

计算log_N_!需要使用斯特林（Stirling）公式，即lnN!=NlnN−N+O(lnN)。因此我们可以得出logN!=_O_(NlogN)的结论。注意，我们现在估算出的是排序所需要进行比较的次数的下限。也就是说，任何排序算法的复杂度不会低于O(NlogN)。

![](https://oss.linklearner.com/the-soul-of-calculation/1.9.png)

图1.9　区分出_N_!种不同序列的大小所需要的比较次数，不能少于包含_N_!个叶节点的二叉树的高度（从根节点到最远的叶节点的节点数）



## 例题

#### 1.1围棋有多复杂？（难度系数1颗星）

棋盘上每一个点最终可以是黑子、白子或者空位三种情况，而棋盘有361个交叉点，因此围棋的变化最多可以有3^361≈2×10^172种情况。这个数当然相当大，大约是2后面跟172个零

整个宇宙中不过才有10^79～10^83个基本粒子。也就是说，如果把每一个基本粒子都变成一个宇宙，再把那么多宇宙中的基本粒子数一遍，数量也没有围棋棋盘上各种变化的总数大。

#### 1.2一句有20个单词的英语语句可以有多少种组合？　（难度系数3颗星）

英语的单词数在10万个以上，这里就算是10万个，20个单词不受限制的组合数是10^100。这个数就是古戈尔（Googol），也比宇宙中基本粒子的数量要多得多。如果随便一句话从理论上讲都有这么多的可能性，那么语音识别就是一个在巨大的多维空间中搜索一个点的问题。

葛立恒数是由美国数学家葛立恒（Ronald Graham）提出的，它大得无法用科学记数法来表示，1980 年被吉尼斯世界纪录确认为当时数学上可以证明存在的最大的数。

1.3见1.3节怎样寻找最好的算法


## 思考题

#### 1.1世界上还有什么产品类似于计算机，是软硬件分离的？（ 难度系数1颗星）

计算机，可定义为任何能计算、有存储能力、受指令控制的机器。其中，硬件是计算单元、存储单元，软件是指令序列。至于为什么要进行软硬件分离？从ENIAC专用计算机到EDVAC通用计算机的历史可以看出，是为了通用，让我们只需要修改或更新软件而无需更换硬件来完成某些需求。

因此，日常生活中很多产品都符合软硬件分离的设计，如智能手机、智能电视、可穿戴电子设备、汽车等。

-   汽车：硬件是发动机、底盘、车身和电气设备；软件是智能AI系统，比如倒车影像位置识别，车身感应器等
-   电视：硬件是外壳、液晶面板、挂架或底座、电路系统；软件是搭载在上面的智能电视软件
-   信件分拣系统：硬件是邮件传输带、分拣手臂、电路控制设备；软件是信件地区智能识别

原文链接：https://blog.csdn.net/qq_38869560/article/details/126431500

#### 1.2如果一个程序只运行一次，在编写它的时候，你是采用最直观但是效率较低的算法，还是依然寻找复杂度最优的算法？（难度系数2颗星）

我会选择在可接受的时间花费内，尽量寻找复杂度更优的算法。不一定追求最优，在效率和代码简易性之间取个折中。

#### 思考题1.3
Q1．将例题1.3的线性复杂度算法写成伪代码。（难度系数2颗星）

```python
def maxSubArray(nums: list) -> (int, list):
    # 区间和
    sub_sum = 0
    # 整个区间的最大值
    max_sum = 0
    # 左边界
    left = 0
    # 右边界
    right = 0
    # 第一个大于零的位置
    p = 0
    # (1) 先在序列中扫描找到第一个大于零的数
    for i in range(len(nums)):
        if nums[i] > 0:
            p = i
            break

    for q in range(p, len(nums)):
        # 计算从0~i的和
        sub_sum += nums[q]
        # （4）比较局部最大和
        if sub_sum > max_sum:
            # 更新max
            max_sum = sub_sum
            # 记录左右边界
            right = q
            left = p
        # （2）当s<0时
        if sub_sum < 0:
            sub_sum = 0
            # （3）从q+1开始往后扫描
            p = q + 1

    return max_sum, nums[left:right + 1]
```


Q2．在一个数组中寻找一个区间，使得区间内的数字之和等于某个事先给定的数字。

（AB、FB、LK等公司的面试题，后面会解答。（难度系数3颗星））

相似题目：[39. 组合总和_M](../explain/39.%20组合总和_M.md)

哈希表存储区间和

```python
def target_sub_array(target: int, nums: list):
    # 定义哈希表{key=sub_sum_value, value=right}
    hash_dict = {}
    # 区间和
    sub_sum = 0
    for q in range(len(nums)):
        sub_sum += nums[q]

        # 情况1：S(1, q) == target
        if sub_sum == target:
            return nums[0:q + 1]

        # 情况2：S(p, q) == target
        # S(p, q) = S(1, q) - S(1, p - 1)
        # S(1, p - 1) = S(1, q) - target
        if sub_sum - target in hash_dict.keys():
            p = hash_dict[sub_sum - target]
            return nums[p + 1:q + 1]

        if sub_sum not in hash_dict.keys():
            hash_dict[sub_sum] = q
```



Q3．在一个二维矩阵中，寻找一个矩形的区域，使其中的数字之和达到最大值。

（例题1.3的变种，硅谷公司真实的面试题。（难度系数4颗星））

[面试题 17.24. 最大子矩阵](https://leetcode.cn/problems/max-submatrix-lcci/)

问题从一维变成了二维，但实质是一样的，同样是再求最大子序和，我们需要将二维转化为一维，对于矩阵的每一列，我们将其加在一起，成为了一维上的一个数，二维矩阵的和转化为了一维数组的和

```python
def get_max_matrix(matrix):
    n = len(matrix)   # 行
    m = len(matrix[0])    # 列
    b = [0] * m    # 每一列的和
    max_sum = -float('inf')
    best_r1, best_c1 = 0, 0
    r1, c1, r2, c2 = 0, 0, 0, 0

    for i in range(n):     # 选择一个起始行
        # 变更子矩阵的时候，都将b数组清0
        for t in range(m):
            b[t] = 0

        for j in range(i, n):     # 结束行
            sub_sum = 0
            for k in range(m):
                b[k] += matrix[j][k]

                # 计算最大子数组
                if sub_sum > 0:
                    sub_sum += b[k]
                else:
                    sub_sum = b[k]
                    best_r1 = i
                    best_c1 = k

                if sub_sum > max_sum:
                    max_sum = sub_sum
                    # 更新值
                    r1 = best_r1
                    c1 = best_c1
                    r2 = j
                    c2 = k

    return max_sum, [r1, c1, r2, c2]
```

参考：https://relph1119.github.io/my-team-learning/#/soul_of_calculation40/task02

#### 思考题1.4

Q1．赛跑问题（GS）。

假定有25名短跑选手比赛争夺前三名，赛场上有五条赛道，一次可以有五名选手同时比赛。比赛并不计时，只看相应的名次。假设选手的发挥是稳定的，也就是说如果约翰比张三跑得快，张三比凯利跑得快，约翰一定比凯利跑得快。最少需要几次比赛才能决出前三名？（在第6章给出了这一问题的解答。（难度系数3颗星））

#td 

Q2．区间排序。

如果有N个区间[_l_1,_r_1],[_l_2,_r_2],…,[_l__N_,_r__N_]，只要满足下面的条件我们就说这些区间是有序的：存在_x__i_∈[_l__i_,_r__i_]，其中_i_=1,2,…,_N_。

比如，[1, 4]、[2, 3]和[1.5, 2.5]是有序的，因为我们可以从这三个区间中选择1.1、2.1和2.2三个数。同时[2, 3]、[1, 4]和[1.5, 2.5]也是有序的，因为我们可以选择2.1、2.2和2.4。但是[1, 2]、[2.7, 3.5]和[1.5, 2.5]不是有序的。

对于任意一组区间，如何将它们进行排序？（难度系数3颗星)

对于同一个问题，可以使用不同的计算机算法。不同算法之间效率的差异可谓天差地别，因此在计算机领域很大一部分工作就是在各种应用中寻找效率更高的算法。当然不同的算法在处理不同规模的问题时所表现的效率可能会有很大的差异，因此在衡量计算机算法的效率时，我们假定要处理的问题规模都非常巨大，近乎无穷。然后，我们需要找到计算量和问题规模_N_之间的函数关系。在计算机科学中，通常我们感兴趣的不是具体的计算量函数，而是它的上界，这个上界可以采用数学中关于函数上界的概念，也就是大_O_的概念来描述。

#td 



