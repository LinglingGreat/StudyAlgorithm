本文是数据结构与算法系列的第3篇

### 什么是数组

**数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。**这句话里有几个关键词需要重点理解。

第一个是线性表（Linear List）。线性表就是数据排成像一条线一样的结构，就像日常生活中经常见到的排队。每个线性表上的数据最多只有前和后两个方向。在数据结构中，数组，链表、队列、栈都是线性表结构。

与它相对立的概念是非线性表，数据之间并不是简单的前后关系，即关系更为复杂，就像人与人之间的朋友关系，A是B的朋友，B是C的朋友，A也是C的朋友，这就形成了一个环，不能用简单的前后关系描述。在数据结构中，二叉树、堆、图等都是非线性表结构。

第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。但这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。

### 如何实现随机访问

计算机会给数组分配一块连续内存空间，内存块的首地址为base_address。每个内存单元都有一个地址，计算机通过这个地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：

a[i]_address = base_address + i * data_type_size

其中 data_type_size 表示数组中每个元素的大小。如果数组中存储的是 int 类型数据，那么 data_type_size 就为 4 个字节。

来看看下列的说法是不是正确的：

链表适合插入、删除，时间复杂度为O(1)；数组适合查找，查找时间复杂度为O(1)。

解答：链表在之后的文章会提到，这里暂且不提。数组是适合查找操作，但是查找的时间复杂度真的是 O(1)吗？实际上，这句话混淆了查找和随机访问的概念。前面讲到，计算机通过寻址公式就能快速地找到数组中的某个元素，所以数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。但是查找≠随机访问，查找是指在数组中找到数据等于某个值的元素，即便是排好序的数组，用二分查找，时间复杂度也是 O(logn)。

上述公式是一维数组的寻址公式，对于 m * n 的二维数组，a \[ i ][ j ] (i < m,j < n)的地址为：

address = base_address + ( i * n + j) * data_type_size

也就是说，二维数组的元素的是按照从左到右，从上到下的顺序存储到内存的。可以把二维数组想象成一个矩阵，先存储第一行数据，再存储第二行，以此类推。



### 低效的“插入”和“删除”

数组中的元素的存储地址是连续的，好处是可以支持随机访问。为了保证内存的连续性，在往数组中插入和删除元素时，我们就得格外注意。比如要往一个数组的第k个位置插入一个元素，为了腾出第k个位置给新的元素，必须将k之后的数据进行搬移。这样插入一个元素的时间复杂度是多少呢？

如果我们是要插入到数组末尾，那很好，不需要进行数据搬移，直接赋值就可以，所以最好情况的时间复杂度为O(1)。如果要插入到数组的开头，需要将整个数组的数据进行搬移，所以最坏的时间复杂度是O(n)。因为每种情况发生的概率相同，所以往数组中插入数据的平均情况时间复杂度为 (1+2+…n)/n=O(n)。是不是很低效？有没有办法优化呢？

思考一下，我们为什么要把k之后的数据都进行搬移呢？因为这样能保持数组原有的数据顺序。如果数组中的数据是有序的，为了保证插入数据之后数组仍然有序，我们别无他法，只能选择数据搬移。

但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。我们就没有必要保持原数组的数据顺序，直接将第 k 位的原数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。也就是把大批的数据搬移变成了两个元素的替换操作，时间复杂度就会降为O(1)。

和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。那删除数据的时间复杂度怎么优化呢？

你可能会想，在不用保证原数组的数据顺序的情况下，直接把数组末尾的数据赋给删除数据的位置不就行了。是的，跟插入时类似，删除元素也可以这么做。但是这个方法只能处理数组无序的情况，还有一种方法可以保证数组仍旧有序。

在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。删除一个元素意味着我们不能再访问它，但仍旧可以继续存储，只要用一个标志标记该元素是否已删除即可。

为了避免剩余的数据会被搬移多次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地将其删除，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的多次的数据搬移。

如果你了解 JVM，你会发现，这不就是 JVM 标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，**很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的**。如果你细心留意，不管是在软件开发还是架构设计中，总能找到某些算法和数据结构的影子。

JVM标记清除算法：

首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。

标记：标记的过程其实就是，遍历所有的GC Roots，然后将所有GC Roots可达的对象标记为存活的对象。GC指的是Garbage Collector，GC Roots指垃圾收集器的对象。

清除：清除的过程将遍历堆中所有的对象，将没有标记的对象全部清除掉。

不足：

1.效率问题。标记和清除效率都不高，但是当知道只有少量垃圾产生时会很高效。

2.空间问题。会产生大量不连续的内存空间碎片。碎片太多可能会导致对象无法分配到足够的连续内存，从而不得不提前触发GC，甚至Stop The World(停止整个程序)。

### 警惕数组的访问越界问题

来看看下面这段代码，有没有什么问题：

```c
int main(int argc, char* argv[]){
    int i = 0;
    int arr[3] = {0};
    for(; i<=3; i++){
        arr[i] = 0;
        printf("hello world\n");
    }
    return 0;
}
```

细心的你会发现，访问越界了啊！数组的大小为3，a[0], a[1], a[2], 而for循环的结束条件是i<=3，即i可以等于3，因为没有a[3]这个元素，那不就会报错嘛！

NoNoNo，这个说法是错误的，这段代码并不会报错！

什么？我不相信！我要去写个Python代码实验一下！

于是，出现了下面的实验过程：

```python
>>> import numpy as np
>>> arr = np.array([0,1,2])
>>> for i in range(4):
...     arr[i]=0
...     print("Hello world")
...
Hello world
Hello world
Hello world
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
IndexError: index 3 is out of bounds for axis 0 with size 3
```

（实际上根本不用写得这么复杂，但是为了保持和C语言代码的相似性，还是这么写了emm）

你看，报错了吧！

对，Python里访问越界的确会报错，Java在遇到访问越界时也会报类似java.lang.ArrayIndexOutOfBoundsException的错误。这是因为Python和Java都会做数组越界检查，而C，C++不会检查！他们把数组越界检查的工作丢给程序员来做！(所以一开始用C写这段代码就是个陷阱，欺负我不懂C...)

实际上，这段代码的运行结果是会无限打印“hello world”，这是为什么呢？(无限迷惑ing)

在 C 语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。所以a[3] 也会被定位到某块不属于数组的内存地址上，而这个地址正好是存储变量 i 的内存地址(？？？看后面，有解释)，那么 a[3]=0 就相当于 i=0，所以就会导致代码无限循环。

数组越界在 C 语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误。

这种情况下，一般都会出现莫名其妙的逻辑错误，debug 的难度非常的大。而且，很多计算机病毒也正是利用到了代码中的数组越界可以访问非法地址的漏洞，来攻击系统，所以写代码的时候一定要警惕数组越界。

解释1：例子中死循环的问题跟编译器分配内存和字节对齐有关，数组3个元素加上一个变量a 。4个整数刚好能满足8字节对齐，所以i的地址恰好跟着a2后面，导致死循环。。如果数组本身有4个元素，则这里不会出现死循环。。因为编译器64位操作系统下 默认会进行8字节对齐，变量i的地址就不紧跟着数组后面了。(来自课程评论)

解释2：函数体内的局部变量存在栈上，且是连续压栈。在Linux进程的内存布局中，栈区在高地址空间，从高向低增长。变量i和arr在相邻地址，且i比arr的地址大，所以arr越界正好访问到i。当然，前提是i和arr元素同类型，否则那段代码仍是未决行为。(来自课程评论)

在网上找到一种解释：

局部变量存储在栈中。编译器按照内存地址递减的方式来给变量分配内存，在局部变量分配空间的顺序跟变量的声明顺序直接相关，同时按照内存由高到低的顺序进行空间分配。i在数组a之前被定义，按照内存地址递减的方式给变量分配内存，那么i是处在最高位地址，a处于低位，如图所示，由低到高存储的分别是：

| a[0] | a[1] | a[2] | i    |
| ---- | ---- | ---- | ---- |

实际上，对于不同的语言、不同的编译器，在内存分配时，会按照内存地址递减或递增的方式进行分配，所以运行结果与语言还有编译器的实现有关系。

### 容器能否完全替代数组？

针对数组类型，很多语言都提供了容器类，比如 Java 中的 ArrayList、C++ STL 中的 vector。在项目开发中，什么时候适合用数组，什么时候适合用容器呢？

这里拿 Java 语言来举例，ArrayList与数组相比，到底有哪些优势呢？

ArrayList 最大的优势就是**可以将很多数组操作的细节封装起来**。比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是**支持动态扩容**。

数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。如果我们申请了大小为 10 的数组，当第 11 个数据需要存储到数组中时，我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后再将新的数据插入。

如果使用 ArrayList，我们就完全不需要关心底层的扩容逻辑，ArrayList 已经帮我们实现好了。每次存储空间不够的时候，它都会将空间自动扩容为 1.5 倍大小。

不过因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。

作为高级语言编程者，是不是数组就无用武之地了呢？当然不是，有些时候，用数组会更合适些。

1.Java ArrayList 无法存储基本类型，比如 int、long(基本数据类型)，需要封装为 Integer、Long 类(包装器类型)，而 Autoboxing、Unboxing(分别指自动装箱、拆箱，装箱就是自动将基本数据类型转换为包装器类型；拆箱就是自动将包装器类型转换为基本数据类型) 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。

2.如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。

3.当要表示多维数组时，用数组往往会更加直观。比如 `Object[][] array`；而用容器的话则需要这样定义：`ArrayList<ArrayList<object> > array`。

总结一下，对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。

### Python数组操作的复杂度

```python
# 创建动态数组
# 不用显式指定数组大小，它会根据实际存储的元素数量自动扩缩容
arr = []

for i in range(10):
    # 在末尾追加元素，时间复杂度 O(1)
    arr.append(i)

# 在中间插入元素，时间复杂度 O(N)
# 在索引 2 的位置插入元素 666
arr.insert(2, 666)

# 在头部插入元素，时间复杂度 O(N)
arr.insert(0, -1)

# 删除末尾元素，时间复杂度 O(1)
arr.pop()

# 删除中间元素，时间复杂度 O(N)
# 删除索引 2 的元素
arr.pop(2)

# 根据索引查询元素，时间复杂度 O(1)
a = arr[0]

# 根据索引修改元素，时间复杂度 O(1)
arr[0] = 100

# 根据元素值查找索引，时间复杂度 O(N)
index = arr.index(666)
```


### 为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？

从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用这个公式：

`a[k]_address = base_address + k * type_size`

但是，如果数组从 1 开始计数，那我们计算数组元素 a[k] 的内存地址就会变为：

`a[k]_address = base_address + (k-1)*type_size`

对比两个公式，我们不难发现，从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。

数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。

上述的说法感觉很有意思，但最主要的原因可能是历史原因。

C 语言设计者用 0 开始计数数组下标，之后的 Java、JavaScript 等高级语言都效仿了 C 语言，或者说，为了在一定程度上减少 C 语言程序员学习 Java 的学习成本，因此继续沿用了从 0 开始计数的习惯。实际上，很多语言中数组也并不是从 0 开始计数的，比如 Matlab。甚至还有一些语言支持负数下标，比如 Python。



### 参考资料

极客时间专栏《数据结构与算法之美》

https://blog.csdn.net/liuhuiyi/article/details/7526889

https://www.cnblogs.com/wang-yaz/p/8516151.html

https://blog.csdn.net/wuzhiwei549/article/details/80563134

https://www.jianshu.com/p/e09a236e9a64

