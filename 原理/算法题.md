## 海量数据处理 - 10亿个数中找出最大的10000个数（top K问题）

分治法（类似于快排）：时间复杂度O(n)  (n+n/2+n/4+...)

堆

https://blog.csdn.net/zyq522376829/article/details/47686867

先拿10000个数建堆，然后一次添加剩余元素，如果大于堆顶的数（10000中最小的），将这个数替换堆顶，并调整结构使之仍然是一个最小堆，这样，遍历完后，堆中的10000个数就是所需的最大的10000个。建堆时间复杂度是O（mlogm），算法的时间复杂度为O（nmlogm）（n为10亿，m为10000）。

​        优化的方法：可以把所有10亿个数据分组存放，比如分别放在1000个文件中。这样处理就可以分别在每个文件的10^6个数据中找出最大的10000个数，合并到一起在再找出最终的结果。

**top K问题**

​        在大规模数据处理中，经常会遇到的一类问题：在海量数据中找出出现频率最好的前k个数，或者从海量数据中找出最大的前k个数，这类问题通常被称为top K问题。例如，在搜索引擎中，统计搜索最热门的10个查询词；在歌曲库中统计下载最高的前10首歌等。

​        **针对top K类问题，通常比较好的方案是分治+Trie树/hash+小顶堆（就是上面提到的最小堆），即先将数据集按照Hash方法分解成多个小数据集，然后使用Trie树或者Hash统计每个小数据集中的query词频，之后用小顶堆求出每个数据集中出现频率最高的前K个数，最后在所有top K中求出最终的top K。**

**eg：有1亿个浮点数，如果找出期中最大的10000个？**

最容易想到的方法是将数据全部排序，然后在排序后的集合中进行查找，最快的排序算法的时间复杂度一般为O（nlogn），如快速排序。但是在32位的机器上，每个float类型占4个字节，1亿个浮点数就要占用400MB的存储空间，对于一些可用内存小于400M的计算机而言，很显然是不能一次将全部数据读入内存进行排序的。其实即使内存能够满足要求（我机器内存都是8GB），该方法也并不高效，因为题目的目的是寻找出最大的10000个数即可，而排序却是将所有的元素都排序了，做了很多的无用功。

​        第二种方法为局部淘汰法，该方法与排序方法类似，用一个容器保存前10000个数，然后将剩余的所有数字——与容器内的最小数字相比，如果所有后续的元素都比容器内的10000个数还小，那么容器内这个10000个数就是最大10000个数。如果某一后续元素比容器内最小数字大，则删掉容器内最小元素，并将该元素插入容器，最后遍历完这1亿个数，得到的结果容器中保存的数即为最终结果了。此时的时间复杂度为O（n+m^2），其中m为容器的大小，即10000。

​        第三种方法是分治法，将1亿个数据分成100份，每份100万个数据，找到每份数据中最大的10000个，最后在剩下的100x10000个数据里面找出最大的10000个。如果100万数据选择足够理想，那么可以过滤掉1亿数据里面99%的数据。100万个数据里面查找最大的10000个数据的方法如下：用快速排序的方法，将数据分为2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大堆个数N小于10000个，就在小的那堆里面快速排序一次，找第10000-n大的数字；递归以上过程，就可以找到第1w大的数。参考上面的找出第1w大数字，就可以类似的方法找到前10000大数字了。此种方法需要每次的内存空间为10^6*4=4MB，一共需要101次这样的比较。

​        第四种方法是Hash法。如果这1亿个书里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的10000个数。

​        第五种方法采用最小堆。首先读入前10000个数来创建大小为10000的最小堆，建堆的时间复杂度为O（mlogm）（m为数组的大小即为10000），然后遍历后续的数字，并于堆顶（最小）数字进行比较。如果比最小的数小，则继续读取后续数字；如果比堆顶数字大，则替换堆顶元素并重新调整堆为最小堆。整个过程直至1亿个数全部遍历完为止。然后按照中序遍历的方式输出当前堆中的所有10000个数字。该算法的时间复杂度为O（nmlogm），空间复杂度是10000（常数）。

**实际运行**：

​        实际上，最优的解决方案应该是最符合实际设计需求的方案，在时间应用中，可能有足够大的内存，那么直接将数据扔到内存中一次性处理即可，也可能机器有多个核，这样可以采用多线程处理整个数据集。

​       下面针对不容的应用场景，分析了适合相应应用场景的解决方案。

**（1）单机+单核+足够大内存**

​        如果需要查找10亿个查询次（每个占8B）中出现频率最高的10个，考虑到每个查询词占8B，则10亿个查询次所需的内存大约是10^9 * 8B=8GB内存。如果有这么大内存，直接在内存中对查询次进行排序，顺序遍历找出10个出现频率最大的即可。这种方法简单快速，使用。然后，也可以先用HashMap求出每个词出现的频率，然后求出频率最大的10个词。

**（2）单机+多核+足够大内存**

​        这时可以直接在内存总使用Hash方法将数据划分成n个partition，每个partition交给一个线程处理，线程的处理逻辑同（1）类似，最后一个线程将结果归并。

​        该方法存在一个瓶颈会明显影响效率，即数据倾斜。每个线程的处理速度可能不同，快的线程需要等待慢的线程，最终的处理速度取决于慢的线程。而针对此问题，解决的方法是，将数据划分成c×n个partition（c>1），每个线程处理完当前partition后主动取下一个partition继续处理，知道所有数据处理完毕，最后由一个线程进行归并。

**（3）单机+单核+受限内存**

​        这种情况下，需要将原数据文件切割成一个一个小文件，如次啊用hash(x)%M，将原文件中的数据切割成M小文件，如果小文件仍大于内存大小，继续采用Hash的方法对数据文件进行分割，知道每个小文件小于内存大小，这样每个文件可放到内存中处理。采用（1）的方法依次处理每个小文件。

**（4）多机+受限内存**

​        这种情况，为了合理利用多台机器的资源，可将数据分发到多台机器上，每台机器采用（3）中的策略解决本地的数据。可采用hash+socket方法进行数据分发。

​        从实际应用的角度考虑，（1）（2）（3）（4）方案并不可行，因为在大规模数据处理环境下，作业效率并不是首要考虑的问题，算法的扩展性和容错性才是首要考虑的。算法应该具有良好的扩展性，以便数据量进一步加大（随着业务的发展，数据量加大是必然的）时，在不修改算法框架的前提下，可达到近似的线性比；算法应该具有容错性，即当前某个文件处理失败后，能自动将其交给另外一个线程继续处理，而不是从头开始处理。
​        top K问题很适合采用MapReduce框架解决，用户只需编写一个Map函数和两个Reduce 函数，然后提交到Hadoop（采用Mapchain和Reducechain）上即可解决该问题。具体而言，就是首先根据数据值或者把数据hash(MD5)后的值按照范围划分到不同的机器上，最好可以让数据划分后一次读入内存，这样不同的机器负责处理不同的数值范围，实际上就是Map。得到结果后，各个机器只需拿出各自出现次数最多的前N个数据，然后汇总，选出所有的数据中出现次数最多的前N个数据，这实际上就是Reduce过程。对于Map函数，采用Hash算法，将Hash值相同的数据交给同一个Reduce task；对于第一个Reduce函数，采用HashMap统计出每个词出现的频率，对于第二个Reduce 函数，统计所有Reduce task，输出数据中的top K即可。
​        直接将数据均分到不同的机器上进行处理是无法得到正确的结果的。因为一个数据可能被均分到不同的机器上，而另一个则可能完全聚集到一个机器上，同时还可能存在具有相同数目的数据。

**以下是一些经常被提及的该类问题。**

（1）有10000000个记录，这些查询串的重复度比较高，如果除去重复后，不超过3000000个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。请统计最热门的10个查询串，要求使用的内存不能超过1GB。
（2）有10个文件，每个文件1GB，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。按照query的频度排序。
（3）有一个1GB大小的文件，里面的每一行是一个词，词的大小不超过16个字节，内存限制大小是1MB。返回频数最高的100个词。
（4）提取某日访问网站次数最多的那个IP。
（5）10亿个整数找出重复次数最多的100个整数。
（6）搜索的输入信息是一个字符串，统计300万条输入信息中最热门的前10条，每次输入的一个字符串为不超过255B，内存使用只有1GB。
（7）有1000万个身份证号以及他们对应的数据，身份证号可能重复，找出出现次数最多的身份证号。

**重复问题**

​        在海量数据中查找出重复出现的元素或者去除重复出现的元素也是常考的问题。针对此类问题，一般可以通过位图法实现。例如，已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。
​        本题最好的解决方法是通过使用位图法来实现。8位整数可以表示的最大十进制数值为99999999。如果每个数字对应于位图中一个bit位，那么存储8位整数大约需要99MB。因为1B=8bit，所以99Mbit折合成内存为99/8=12.375MB的内存，即可以只用12.375MB的内存表示所有的8位数电话号码的内容。

https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/02.01.html

### 假设淘宝一天有 5 亿条成交数据，求出销量最高的 100 个商品。

方法：采用 Hash 映射+Hash 统计+排序算法
先用哈希统计每个商品的成交次数，可以把 5 亿个数据分组存放，比如放在5000 个文件中。这样就可以分别在每个文件的 10^6 个数据中，用哈希+排序算法，求出每个区域内前 100 个频率最高的商品，最后求出所有记录中出现频率最高的前100 个商品。

## 合法()表达式

**一个合法的表达式由()包围，()可以嵌套和连接，如(())()也是合法表达式；现在有6对()，它们可以组成的合法表达式的个数为多少？ **

132

https://www.zhihu.com/question/25072237

$C_{2n}^n - C_{2n}^{n+1}=\frac 1{n+1}C_{2n}^n$

## 捡装备

**《绝地求生》游戏中，护甲和防弹衣都有三种等级，分别记为1,2,3. 每次你只能捡起没有的装备，或者将低等级的装备升级到高等级，问从什么都没有到“三级护甲三级防弹衣”，有多少种升级路线？用有序数对（防弹衣，护甲）来表示当前状态，二者取值0-3，比如（0,0）->（0,1）->（0,3）->（3,3）为一种升级策略 **

![img](https://images2018.cnblogs.com/blog/1171284/201804/1171284-20180408185008311-489267418.png)

## 调色盘特殊颜料

**假设有一种无色的特殊颜料，与红色颜料混合后会变紫色，与黄色颜料混合会变为绿色，与红色、黄色颜料一起混合会变为黑色，发生颜色变化需要1小时。现有700瓶特殊颜料，其中一瓶已经变质，不管与什么颜料混合都会变为白色。只有一小时时间, 最少需要多少个调色盘才能找出变质的特殊颜料？ **

10个调色盘  ，分别当作10个二进制位。 

  700瓶颜料从1到700编号，写成二进制形式，对于每一瓶颜料，在其二进制为1的位所对应的调色盘上加入。 

  一小时后，按照变成白色该调色盘对应位为1的原则，写出一个二进制数就是变质颜料的编号。

比如第5瓶颜料是变质颜料，其二进制编码是101，就在第一个和第三个调色板上加入该颜料，最后结果是第一个调色板和第三个调色板都变白了，其余没有。这题本质就是问至少需要多少个二进制位更够表示出所有的编号，显然，对700进行二进制编码至少需要10个位。 

相似题：

**8瓶酒一瓶有毒，用人测试。每次测试结果8小时后才会得出，而你只有8个小时的时间。问最少需要（）人测试？**

本题的解题思路是二进制：
将毒酒编号为0~7，有三个人ABC根据每一个值转化的二进制：
十进制：  0   1   2   3   4   5   6   7
二进制：000 001 010 011 100 101 110 111
A和对应二进制从左往右数第一位为1的，B喝所有第二位为1的，C喝所有第三位为1的
根据ABC中毒情况还原二进制，此题得解。

## 直线段截成3段，可构成三角形的概率 

假设把绳子等分为两段 分别为AB BC 那么要截为3段且能组成三角形 选取的任意点a b必须分别从AB BC 段选出

0.5✕0.5=0.25

## 砝码称重

**珠宝商甲需要去鉴定一批41克以下的宝石（可能是41克以下不包括41克的任意重量），他只能携带一个天平和四个砝码去称重，请问他会携带哪些重量的砝码？ **

```
1 3 9 27
1 10 20 30 40
1 4 16 32
1 3 10 21
```

A，

解法1：1=1，2=3-1，3=3，4=3+1，5=9-3-1，6=9-3，7=9+1-3，...

解法2：3、9、27可以组成3的倍数，多一克可以加1，少一克可以减1 选A 

解法3：带了四个砝码，即可以用四个进制位表示。设进制数为n，那么$n^0 +n^1+ n^2+ n^3>=40$。用等比数列的公式可以求得n=3。所以四个砝码分别是$1(3^0)、3(3^1)、9(3^2)、27(3^3)$。选A 

## 打比赛

**小组赛，每个小组有5支队伍，互相之间打单循环赛，胜一场3分，平一场1分，输一场不得分，小组前三名出线。平分抽签。问一个队最少拿()分就有理论上的出线希望： **

如果的1分，说明输了三场，平了一场，肯定有三个队伍赢了

如果的2分，说明平了两场，如果另外两个队也是平了，也是两分，就有可能

## 0 到 9999 这 1 万个数中有多少个数字 7



两种解法，

第一种 将出现 7 的情况分为四种，分别是个位、十位、百位、千位上出现,固定某一位为7，其他三位共有 10\*10\*10=1000种不同数字 ，因此答案为4000 

第二种 四位数一共10000个，每个有4个数字不足四位的前面补零，这样共有40000个数字，每个数字出现频率相同，故有4000个数  

## 查找出现次数超过一半的数字

**春节期间小明使用微信收到很多个红包，非常开心。在查看领取红包记录时发现，某个红包金额出现的次数超过了红包总数的一半。请帮小明找到该红包金额。写出具体算法思路和代码实现，要求算法尽可能高效给定一个红包的金额数组 gifts 及它的大小 n，请返回所求红包的金额。没找到，返回 0。**
解析：这是典型的算法题，可以使用部分快速排序算法求解，通过反复调用 partition函数来实现。
先在数组中随机选取一个数字，而后通过 Partition 函数返回该数字在数组中的索引 index，1）如果 index 刚好等于 n/2，则这个数字便是数组的中位数，即是要求的数；2）如果 index 大于 n/2，则中位数肯定在 index 的左边，反之在右边，缩小搜寻区域，可以减少一半排序时间。时间复杂度来讲，最优情况下为 O(1)，平均情下为 O(n),最坏的时间复杂度依然为 O(n^2)，空间复杂度为 O(1)。

## 随机播放

**如何实现一个随机播放音乐，要求能够方便查看上一首播放的是什么，方便交换即将播放的歌曲顺序**
解析：实现随机播放音乐：先对每首歌曲附上编号，使用洗牌算法来打乱歌曲顺序，Python 的 random 库中，shuffle 函数可以实现洗牌算法，并记录歌曲编号。

## 不使用第三个数（临时变量）交换两个整形数

1） 算术运算

```
int a,b;
a=10;b=12;
a=b-a; //a=2;b=12
b=b-a; //a=2;b=10
a=b+a; //a=10;b=10
```

它的原理是：把a、b看做数轴上的点，围绕两点间的距离来进行计算。
具体过程：第一句“a=b-a”求出ab两点的距离，并且将其保存在a中；第二句“b=b-a”求出a到原点的距离（b到原点的距离与ab两点距离之差），并且将其保存在b中；第三句“a=b+a”求出b到原点的距离（a到原点距离与ab两点距离之和），并且将其保存在a中。完成交换。

2) 使用位运算求解：
a^=b
b^=a
a^=b

```
int a=10,b=12; //a=1010^b=1100;
a=a^b; //a=0110^b=1100;
b=a^b; //a=0110^b=1010;
a=a^b; //a=1100=12;b=1010;
```

此算法能够实现是由异或运算的特点决定的，通过异或运算能够使数据中的某些位翻转，其他位不变。这就意味着**任意一个数与任意一个给定的值连续异或两次，值不变**。
即：a ^b ^ b=a。将a=a ^ b代入b=a^ b则得b=a^ b^ b=a;同理可以得到a=b^ a^ a=b;轻松完成交换。

## 字符移动

**小 Q 最近遇到了一个难题：把一个字符串的大写字母放到字符串的后面，各个字符的相对位置不变，且不能申请额外的空间。你能帮帮小 Q 吗？ 例如：OkhaoPingCeilXu 转换后 khaoingeiluOPCX**

需要注意必须是 O(1)的空间复杂度，所以不能引入新列，思路是把大写字母移动到字符串末尾。时间的复杂度为 O(n^2)
时间的复杂度为 O(n^2)

```
include <iostream>
include <algorithm>
include <string>
using namespace std;
//交换值的函数
void Swop(char& a,char& b){
char c=a;
a=b;
b=c;
}
int main(void) {
string s;
//判断是否是大写字母，是的话就进行两两交换，换到字符串末尾
while(cin>>s){
int len = s.length();
int End = len;
for(int i=0;i<End;i++){
if(s[i]>='A' && s[i]<='Z'){
for(int j=i;j<len-1;j++)
Swop(s[j],s[j+1]);
End--;
i--;
}
}
cout<<s<<endl;
}
return 0;
}
```

## 甲乙射箭，甲比乙多的概率

**甲乙两个人比试射箭，两人射术水平一样，单次射中的概率都为 0.5。如果甲射了 101 箭，而乙射了 100 箭，求甲射中次数比乙射中次数多的概率是？**
A、 1/4
B、 1/2
C、 3/4
D、 1/3
答案：B
解析：
前 100 次中，分为甲多（A）、甲乙一样多（B）、乙多（C）三种情况（三个事件互斥），因为水平一样，则事件 A、C 发生概率一样 
P（甲射中次数多）=P（前 100 次甲射中次数多）+P（前 100 次一样多且最后一次甲射中）=[P（前 100 次甲多）+P（前 100 次乙多）]/2+P（前 100 次一样多）*P（最后一次甲射中）=P（A+B+C）/2=1/2

## N 个节点的二叉树有多少种形态？（卡特兰数）

Catalan 数，公式为：m=C(N,2N) /(N+1)，Catalan 数是组合数学中一个常在计数问题中出现的数列，则一共有 m=C(N,2N) /(N+1)种形态

详解

https://blog.csdn.net/adminabcd/article/details/46672759

https://blog.csdn.net/Hackbuteer1/article/details/7450250

## 查找和为某个数的两个元素

**给定一个整数 sum，从有 N 个有序元素的数组中寻找元素 a、b，使得 a+b 的结果最接近 sum，最快的平均时间复杂度是____。**
A、O(N^2)
B、O(log N)
C、O(N)
D、O(N^3)
E、O(NLogN)
F、不确定
答案：C
解析：
设置两个指针，一个位于最小值，一个位于最大值，即链表的头尾；取指针指向数值，进行加总并和 sum 比较，和大于 sum 时：尾指针向前移动一位；和小于sum 时：首指针向后移动一位，直到差值最小为止，遍历一遍的时间复杂度为 O(N)

## 组成数字1到2,000,000的所有数字的各位的总和是

只要a+b没有进位,(a+b)的数字之和=a的数字之和+b的数字之和。按照此道理,把1和1,999,998一组,和为55，2和1,999,997,和为55...999，999和1,000,000 一组,和为55,所以这里共999,999个55,加上1,999,999和2,000,000为 1,000,000*55+2=55,000,002

或者，

用数学期望做

首先把所有数字前面补上0。先只考虑0,000,000 ~ 1,999,999

随机抽取一个数 各位数字的期望是

0.5 | 4.5 4.5 4.5 | 4.5 4.5 4.5

和为 27.5，一共有2,000,000个数，27.5 * 2,000,000 = 55,000,000

加上2,000,000这个数，所以和为55,000,002



实战测试题（一）
--------

假设猎聘网有 10 万名猎头顾问，每个猎头顾问都可以通过做任务（比如发布职位），来积累积分，然后通过积分来下载简历。假设你是猎聘网的一名工程师，如何在内存中存储这 10 万个猎头 ID 和积分信息，让它能够支持这样几个操作：

根据猎头的 ID 快速查找、删除、更新这个猎头的积分信息；

查找积分在某个区间的猎头 ID 列表；

查询积分从小到大排在第 x 位的猎头 ID 信息；

查找按照积分从小到大排名在第 x 位到第 y 位之间的猎头 ID 列表。

### 相关章节

17 | 跳表：为什么 Redis 一定要用跳表来实现有序集合？

20 | 散列表（下）：为什么散列表和链表经常会一起使用？

25 | 红黑树：为什么工程中都用红黑树这种二叉树？

### 题目解析

这个问题既要通过 ID 来查询，又要通过积分来查询，所以，对于猎头这样一个对象，我们需要将其组织成两种数据结构，才能支持这两类操作。

我们按照 ID，将猎头信息组织成散列表。这样，就可以根据 ID 信息快速地查找、删除、更新猎头的信息。我们按照积分，将猎头信息组织成跳表这种数据结构，按照积分来查找猎头信息，就非常高效，时间复杂度是 O(logn)。

我刚刚讲的是针对第一个、第二个操作的解决方案。第三个、第四个操作是类似的，按照排名来查询，这两个操作该如何实现呢？

我们可以对刚刚的跳表进行改造，每个索引结点中加入一个 span 字段，记录这个索引结点到下一个索引结点的包含的链表结点的个数。这样就可以利用跳表索引，快速计算出排名在某一位的猎头或者排名在某个区间的猎头列表。

实际上，这些就是 Redis 中有序集合这种数据类型的实现原理。在开发中，我们并不需要从零开始代码实现一个散列表和跳表，我们可以直接利用 Redis 的有序集合来完成。

实战测试题（二）
--------

电商交易系统中，订单数据一般都会很大，我们一般都分库分表来存储。假设我们分了 10 个库并存储在不同的机器上，在不引入复杂的分库分表中间件的情况下，我们希望开发一个小的功能，能够快速地查询金额最大的前 K 个订单（K 是输入参数，可能是 1、10、1000、10000，假设最大不会超过 10 万）。如果你是这个功能的设计开发负责人，你会如何设计一个比较详细的、可以落地执行的设计方案呢？

为了方便你设计，我先交代一些必要的背景，在设计过程中，如果有其他需要明确的背景，你可以自行假设。

数据库中，订单表的金额字段上建有索引，我们可以通过 select order by limit 语句来获取数据库中的数据；

我们的机器的可用内存有限，比如只有几百 M 剩余可用内存。希望你的设计尽量节省内存，不要发生 Out of Memory Error。

### 相关章节

12 | 排序（下）：如何用快排思想在 O(n) 内查找第 K 大元素？

28 | 堆和堆排序：为什么说堆排序没有快速排序快？

29 | 堆的应用：如何快速获取到 Top 10 最热门的搜索关键词？

### 题目解析

解决这个题目的基本思路我想你应该能想到，就是借助归并排序中的合并函数，这个我们在排序（下）以及堆的应用那一节中讲过。

我们从每个数据库中，通过 select order by limit 语句，各取局部金额最大的订单，把取出来的 10 个订单放到优先级队列中，取出最大值（也就是大顶堆堆顶数据），就是全局金额最大的订单。然后再从这个全局金额最大订单对应的数据库中，取出下一条订单（按照订单金额从大到小排列的），然后放到优先级队列中。一直重复上面的过程，直到找到金额前 K（K 是用户输入的）大订单。

从算法的角度看起来，这个方案非常完美，但是，从实战的角度来说，这个方案并不高效，甚至很低效。因为我们忽略了，数据库读取数据的性能才是这个问题的性能瓶颈。所以，我们要尽量减少 SQL 请求，每次多取一些数据出来，那一次性取出多少才合适呢？这就比较灵活、比较有技巧了。一次性取太多，会导致数据量太大，SQL 执行很慢，还有可能触发超时，而且，我们题目中也说了，内存有限，太多的数据加载到内存中，还有可能导致 Out of Memory Error。

所以，一次性不能取太多数据，也不能取太少数据，到底是多少，还要根据实际的硬件环境做 benchmark 测试去找最合适的。

实战测试题（三）
--------

我们知道，CPU 资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致 CPU 频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。

当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？

### 相关章节

09 | 队列：队列在线程池等有限资源池中的应用

### 题目解析

这个问题的答案涉及队列这种数据结构。队列可以应用在任何有限资源池中，用于排队请求，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过 “队列” 这种数据结构来实现请求排队。

这个问题的具体答案，在队列那一节我已经讲得非常详细了，你可以回去看看，这里我就不赘述了。

实战测试题（四）
--------

通过 IP 地址来查找 IP 归属地的功能，不知道你有没有用过？没用过也没关系，你现在可以打开百度，在搜索框里随便输一个 IP 地址，就会看到它的归属地。

这个功能并不复杂，它是通过维护一个很大的 IP 地址库来实现的。地址库中包括 IP 地址范围和归属地的对应关系。比如，当我们想要查询 202.102.133.13 这个 IP 地址的归属地时，我们就在地址库中搜索，发现这个 IP 地址落在 [202.102.133.0, 202.102.133.255] 这个地址范围内，那我们就可以将这个 IP 地址范围对应的归属地 “山东东营市” 显示给用户了。

[202.102.133.0, 202.102.133.255] 山东东营市

[202.102.135.0, 202.102.136.255] 山东烟台

[202.102.156.34, 202.102.157.255] 山东青岛

[202.102.48.0, 202.102.48.255] 江苏宿迁

[202.102.49.15, 202.102.51.251] 江苏泰州

[202.102.56.0, 202.102.56.255] 江苏连云港

在庞大的地址库中逐一比对 IP 地址所在的区间，是非常耗时的。假设在内存中有 12 万条这样的 IP 区间与归属地的对应关系，如何快速定位出一个 IP 地址的归属地呢？

### 相关章节

15 | 二分查找（上）：如何用最省内存的方式实现快速查找功能？

16 | 二分查找（下）：如何快速定位 IP 对应的省份地址？

### 题目解析

这个问题可以用二分查找来解决，不过，普通的二分查找是不行的，我们需要用到二分查找的变形算法，查找最后一个小于等于某个给定值的数据。不过，二分查找最难的不是原理，而是实现。要实现一个二分查找的变形算法，并且实现的代码没有 bug，可不是一件容易的事情，不信你自己写写试试。

关于这个问题的解答以及写出 bug free 的二分查找代码的技巧，我们在二分查找（下）那一节有非常详细的讲解，你可以回去看看，我这里就不赘述了。

实战测试题（五）
--------

假设我们现在希望设计一个简单的海量图片存储系统，最大预期能够存储 1 亿张图片，并且希望这个海量图片存储系统具有下面这样几个功能：

存储一张图片及其它的元信息，主要的元信息有：图片名称以及一组 tag 信息。比如图片名称叫玫瑰花，tag 信息是 {红色，花，情人节}；

根据关键词搜索一张图片，比如关键词是 “情人节 花”“玫瑰花”；

避免重复插入相同的图片。这里，我们不能单纯地用图片的元信息，来比对是否是同一张图片，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。

我们希望自主开发一个简单的系统，不希望借助和维护过于复杂的三方系统，比如数据库（MySQL、Redis 等）、分布式存储系统（GFS、Bigtable 等），并且我们单台机器的性能有限，比如硬盘只有 1TB，内存只有 2GB，如何设计一个符合我们上面要求，操作高效，且使用机器资源最少的存储系统呢？

### 相关章节

21 | 哈希算法（上）：如何防止数据库中的用户信息被脱库？

22 | 哈希算法（下）：哈希算法在分布式系统中有哪些应用？

### 题目解析

这个问题可以分成两部分，第一部分是根据元信息的搜索功能，第二部分是图片判重。

第一部分，我们可以借助搜索引擎中的倒排索引结构。关于倒排索引我会在实战篇详细讲解，我这里先简要说下。

如题目中所说，一个图片会对应一组元信息，比如玫瑰花对应 {红色，花，情人节}，牡丹花对应{白色，花}，我们可以将这种图片与元信息之间的关系，倒置过来建立索引。“花” 这个关键词对应 {玫瑰花，牡丹花}，“红色” 对应 {玫瑰花}，“白色” 对应 {牡丹花}，“情人节” 对应{玫瑰花}。

当我们搜索 “情人节 花” 的时候，我们拿两个搜索关键词分别在倒排索引中查找，“花”查找到了 {玫瑰花，牡丹花}，“情人节” 查找到了{玫瑰花}，两个关键词对应的结果取交集，就是最终的结果了。

第二部分关于图片判重，我们要基于图片本身来判重，所以可以用哈希算法，对图片内容取哈希值。我们对哈希值建立散列表，这样就可以通过哈希值以及散列表，快速判断图片是否存在。

我这里只说说我的思路，这个问题中还有详细的内存和硬盘的限制。要想给出更加详细的设计思路，还需要根据这些限制，给出一个估算。详细的解答，我都放在哈希算法（下）那一节里了，你可以自己回去看。

实战测试题（六）
--------

我们知道，散列表的查询效率并不能笼统地说成是 O(1)。它跟散列函数、装载因子、散列冲突等都有关系。如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降。

在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果我们使用的是基于链表的冲突解决方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从 O(1) 急剧退化为 O(n)。

如果散列表中有 10 万个数据，退化后的散列表查询的效率就下降了 10 万倍。更直观点说，如果之前运行 100 次查询只需要 0.1 秒，那现在就需要 1 万秒。这样就有可能因为查询操作消耗大量 CPU 或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。这也就是散列表碰撞攻击的基本原理。

如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？

### 相关章节

18 | 散列表（上）：Word 文档中的单词拼写检查功能是如何实现的？

19 | 散列表（中）：如何打造一个工业级水平的散列表？

### 题目解析

我经常把这道题拿来作为面试题考察候选人。散列表可以说是我们最常用的一种数据结构了，编程语言中很多数据类型，都是用散列表来实现的。尽管很多人能对散列表都知道一二，知道有几种散列表冲突解决方案，知道散列表操作的时间复杂度，但是理论跟实践还是有一定距离的。光知道这些基础的理论并不足以开发一个工业级的散列表。

所以，我在散列表（中）那一节中详细给你展示了一个工业级的散列表要处理哪些问题，以及如何处理的，也就是这个问题的详细答案。

这六道题你回答得怎么样呢？或许你还无法 100% 回答正确，没关系。其实只要你看了解析之后，有比较深的印象，能立马想到哪节课里讲过，这已经说明你掌握得不错了。毕竟想要完全掌握我讲的全部内容还是需要时间沉淀的。对于这门课的学习，你一定不要心急，慢慢来。只要方向对了就都对了，剩下就交给时间和努力吧！

通过这套题，你对自己的学习状况应该有了一个了解。从专栏开始到现在，三个月过去了，我们的内容也更新了大半。你在专栏开始的时候设定的目标是什么？现在实施得如何了？你可以在留言区给这三个月的学习做个阶段性学习复盘。重新整理，继续出发！